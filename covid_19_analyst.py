# -*- coding: utf-8 -*-
"""COVID_19_ANALYST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T23ma3OEv33fDKguBvR_m5tOB4sDcEdG

# <center><ins>COVID19 ANALYTICS</ins></center>

Una entidad gubernamental responsable de la gestión de la salud en un país enfrenta el desafío de comprender y analizar la propagación del COVID-19 para tomar decisiones informadas y eficaces en la gestión de la pandemia. Como científico de datos, tu tarea es analizar los datos relacionados con el COVID-19 y presentar insights a través de visualizaciones que respondan a las siguientes preguntas clave:

**Preguntas:**

1. ¿Cómo ha evolucionado el Covid-19 en **México-Perú-Colombia** en comparación con el impacto observado a nivel global?

2. ¿Cuál ha sido la evolución de los nuevos casos diarios reportados de Covid-19 en **Colombia** a lo largo del tiempo?

3. ¿Cuál es la evolución del índice de letalidad del Covid-19 en **Colombia**, comparado con los países con los índices históricos más elevados?

## 1. Importando Nuestros Datasets
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
from io import StringIO
import requests

## Guardando en variables nuestros datasets
df_covid = pd.read_csv(StringIO(requests.get("https://covid19.who.int/WHO-COVID-19-global-data.csv").text))
df_population = pd.read_excel('https://raw.githubusercontent.com/davidcarrillo10288/Covid-19-analysis/master/WPP2022_GEN_F01_DEMOGRAPHIC_INDICATORS_COMPACT_REV1.xlsx', sheet_name=0, skiprows=16)
df_population = df_population[df_population['Year'] == 2019]

## Validando nuestro dataset Covid19
df_covid.head(8)

df_population.head()

"""## 2. Análisis Exploratorio de Datos (EDA)

---

### 2.1 EDA Base Covid World Health Organization

---
"""

## Realizamos una exploracion de nuestros datos
df_covid.info()

"""* Observamos que tenemos 8 columnas y 55680 registros.
* Notamos la presencia de valores nulos, esto se analizará posteriormente.
* Los tipos de datos son 3, (Objeto, entero, flotante).

### 2.1.1 Tratamiento de Valores nulos

---
"""

## Importamos la Librería Missingno, para poder visualizar nuestros valores nulos presentes
import missingno as msno

## Verificando la presencia de valores nulos en el dataset
df_covid.isnull().sum().sort_values(ascending=False)

"""* Observamos que existe presencia de datos nulos.
* Tenemos que validar la relación que existen entre datos nulos.
* La presencia de datos nulos en variables como 'country_code' y 'country', nos generan alerta, ya que de no tener datos para estas columnas, esto no generaría valor para el dataset.
* Se evaluará la posibilidad de eliminar ciertas filas que tengan presencia de valores nulos.
"""

## Mostrando visualmente la cantidad de nulos para cada variable, con respecto al total de datos
msno.bar(df_covid, color=sns.color_palette("inferno"),label_rotation=15)
plt.title('GRÁFICO DE BARRAS - VALORES NULOS', fontdict={'fontsize': 20, 'fontweight': 'bold', 'color': '#552E75'})
# plt.tight_layout()
plt.show()

"""> * **Si bien es cierto, hay algunas variables que tienen fuerte cantidad de nulos, considerar que puede ser simplemente que se tenga que modificar por valores de cero.**
> * **Las variables 'country_code' y 'country', tienen presencia de nulos, pero en cantidades pequeñas.**

**OBS: ELIMINADO LOS VALORES NULOS DE COUNTRY_CODE**

---
"""

index_country_code_nulos = df_covid[df_covid['Country_code'].isnull()].index
print(index_country_code_nulos)

df_covid_limpio = df_covid.drop(index_country_code_nulos)
df_covid_limpio

df_covid_limpio.shape, df_covid.shape

"""* Validamos que se eliminaron los valores nulos de forma correcta

### 2.1.2 Tratamiento de Duplicados
"""

## Verificando presencia de duplicados en el dataframe original
df_covid.duplicated()

## Cantidad de datos duplicados en todo el dataframe original
df_covid.duplicated().sum()

"""**Observamos que nuestro DF original no presenta duplicados**

### 2.1.3 Convirtiendo los Valores Nulos existentes en Valor numérico 'Cero'

---
"""

df_covid_limpio.info()

"""* Notamos que las variables que son del tipo Object, ya no tienen datos nulos a excepción de la variable 'WHO_region'
* Al analizar esta variable 'WHO_region', observamos que es solo una etiqueta que representa la región al que pertenece cada país. En este caso, consideramos no relevante para nuestros análisis posteriores. Se decide prescindir de esta variable.
* Las otras variables restantes, notamos que son datos numéricos, enteros y flotantes.
* Al visualizar el dataframe notamos que estas variables numéricas, representan datos de casos y/o decesos, por consiguientes estos datos nulos en sí son valores CERO.
* Se decide realizar el cambio de valores nulos por el valor CERO. Se realiza el proceso .replace(np.nan,0)
"""

## Eliminamos la columna 'WHO_region'
df_covid_limpio = df_covid_limpio.drop(columns= 'WHO_region')
df_covid_limpio.head()

## Reemplazamos los valores nulos restantes por el valor CERO
df_covid_limpio = df_covid_limpio.replace(np.nan,0)
df_covid_limpio.head(10)

## Validamos si seguimos teniendo presencia de valores nulos
df_covid_limpio.info()

df_covid_limpio.isnull().sum()

"""> * **No tenemos presencia de datos nulos en nuestro dataframe Limpio**

### 2.1.4 Convirtiendo la columna 'Date_reported' a un formato datetime

---
"""

## Se utiliza el método de pandas to_datetime
df_covid_limpio['Date_reported'] = pd.to_datetime(df_covid_limpio['Date_reported'])
df_covid_limpio['Date_str'] = df_covid_limpio['Date_reported'].dt.strftime('%Y-%m-%d')
df_covid_limpio.info()

"""### 2.1.4 Convirtiendo la columna ''New_cases', 'New_deaths' a un formato int64

---
"""

## Transformando los datos a valores enteros
df_covid_limpio['New_cases'] = df_covid_limpio['New_cases'].astype('int64')
df_covid_limpio['New_deaths'] = df_covid_limpio['New_deaths'].astype('int64')

## Verificando nuevamente los tipos de datos en el dataframe
df_covid_limpio.info()

"""## 2.1.7 Tratamiento de Valores Atípicos

---

"""

## Analizamos los valores estadísticos principales de nuestro dataframe
df_covid_limpio.describe()

"""* Observamos que si bien no hay valores nulos, hay ciertas columnas que nos indican datos confusos
* En las columnas New_cases y New_deaths, visualizamos que tienen valores minimos negativos, lo cual no es posible, se evaluará posteriormente

"""

## Creando un backup
df_covid_limpio_2 = df_covid_limpio.copy()

## Verificando lo sucedido con la columna New_deaths, cantidad de valores negativos
(df_covid_limpio['New_deaths']<0).sum()

# Encontrando las filas donde la columna New_deaths tiene valores negativos
filas_con_negativos = list(df_covid_limpio.index[df_covid_limpio['New_deaths'] <0])

# Muestra las filas que contienen 'inf' en la columna lethality_rate
print("Filas con valores negativos en la columna New_deaths:", filas_con_negativos)

## Observando en mi dataframe, estas filas para ver que sucedió porque se generaron valores negativos
df_covid_limpio_2.loc[[2726, 5868, 8675, 10102, 10374, 20371, 24007, 38520, 38765, 41152, 45403, 49249],:]

# Reemplazar valores negativos por cero
df_covid_limpio_2['New_deaths'] = df_covid_limpio_2['New_deaths'].apply(lambda x: max(x, 0))

# Aplicar la transformación logarítmica
df_covid_limpio_2['New_deaths_log'] = np.log(df_covid_limpio_2['New_deaths'] + 1)

print(df_covid_limpio_2)

"""* Para evitar obtener valores inf (infinito) al aplicar una transformación logarítmica en un DataFrame que contiene valores negativos y ceros, primero se deben manejar adecuadamente estos valores.

* Una técnica consiste en usar una función lambda para reemplazar los valores negativos con NaN (Not a Number) y luego aplicar el logaritmo natural sumando 1 a los valores positivos.

* Estas estrategias permiten manejar adecuadamente los valores especiales y realizar la transformación logarítmica de manera efectiva, manteniendo la integridad del análisis de datos.

### 2.1.8 Dataframe Transformado


---
"""

## Reasignando la variable y corrigiendo el índice
df_covid_limpio_final = df_covid_limpio_2.reset_index(drop=True)
df_covid_limpio_final

"""* Conseguimos el dataframe **df_covid_limpio_final**, el cual ya esta trabajado, libre de valores nulos y atípicos. Se continuará trabajando de ahora en adelante con este dataframe.

### 2.2 EDA Base Population United Nations
"""

## Vista general del dataframe
df_population

## Dimensiones de nuestro dataframe
df_population.shape

"""* Validamos que tenemos 65 columnas y 286 filas
* Evaluaremos que columnas serán más importantes para nuestros análisis
"""

## Realizamos una visión general de nuestros tipos de datos
df_population.info()

## Verificamos presencia de valores nulos en nuestros datos
df_population.isnull().sum().sort_values(ascending=False)

"""* Observamos que tenemos 4 variables con presencia de valores nulos"""

## Verificamos detalladamente los nombres de las columnas existentes en el dataframe
df_population.columns

"""* En este punto, notamos que tenemos muchas variables en nuestro dataframe, realizamos un análisis rápido y validamos que columnas nos servirán para nuestros análisis posteriores.

* Decidimos mantener en el dataframe sólo las siguientes columnas: ***['ISO2 Alpha-code','Total Population, as of 1 July (thousands)','Male Population, as of 1 July (thousands)','Female Population, as of 1 July (thousands)','Population Density, as of 1 July (persons per square km)','Life Expectancy at Birth, both sexes (years)']***
"""

## Generamos un nuevo dataframe que contenga solo las variables elegidas
df_population_mod = df_population[['ISO2 Alpha-code','Total Population, as of 1 July (thousands)',
                                  'Male Population, as of 1 July (thousands)','Female Population, as of 1 July (thousands)',
                                  'Population Density, as of 1 July (persons per square km)',
                                  'Life Expectancy at Birth, both sexes (years)']]
df_population_mod

## Realizamos un renombre de las variables
df_population_mod = df_population_mod.rename(columns={
                            'ISO2 Alpha-code': 'Country_code',
                            'Total Population, as of 1 July (thousands)': 'Total_Population',
                            'Male Population, as of 1 July (thousands)': 'Male_Population',
                            'Female Population, as of 1 July (thousands)': 'Female_Population',
                            'Population Density, as of 1 July (persons per square km)': 'Population_Density',
                            'Life Expectancy at Birth, both sexes (years)': 'Life_Expectancy'
                            })
df_population_mod

## Verificamos el tipo de dato que tiene cada variable
df_population_mod.info()

"""* Notamos que en este momento todas las variables son del tipo **'object'**. Sin embargo, nos damos cuenta que hay variables que son númericas, se evaluará posteriormente.

### 2.2.1 Tratamiento de Valores Nulos

---
"""

## Validamos cantidad de nulos existentes para este nuevo dataframe
df_population_mod.isnull().sum()

## Verificamos los tipos de categorías que tenemos en nuestra variable 'Country_code'
df_population_mod['Country_code'].unique()

"""* Observamos que tiene valores ***nan***
* Luego de analizar detenidamente a que corresponden estos valores nulos, se visualiza que la variable **'Country_code'** esta relacionada con la variable ***'Region, subregion, country or area *'*** del dataframe original de pupulation, y en las filas donde esta la variable no se coloca un país sino una región o subregión, no figuran códigos de país. Debido a esto no podríamos corregirlo, ya que tenemos que tener un país especifico con su código especifico, decidimos eliminar las filas con valores nulos de la variable **'Country_code'**
"""

## Eliminado las filas con valores nulos en la variable 'Country_code' - Método 1
df_population_mod_notna = df_population_mod[df_population_mod['Country_code'].notna()]
df_population_mod_notna

## Eliminado las filas con valores nulos en la variable 'Country_code' - Método 2
df_population_mod_dropna = df_population_mod.dropna(subset=['Country_code'])
df_population_mod_dropna

## Convirtiendo valores '...' a nan, presentes en nuestras variables Male_Population, Female_Population y Life_Expectancy
df_population_mod_dropna = df_population_mod_dropna.replace('...',np.nan)
df_population_mod_dropna

"""* Se decide eliminar estos nuevos valores nulos, solo son 3 datos.
* En este caso la variable como Life_Expectancy, puede depender de diversos factores y puede ser difícil de imputar con precisión, la imputación podría no ser adecuada. Asimismo, variables demográficas como Male_Population y Female_Population pueden tener relaciones complejas que no se pueden capturar fácilmente con imputación.
"""

## Se decide eliminar los nuevos valores nulos obtenidos
df_population_mod_dropna = df_population_mod_dropna.dropna(subset=['Male_Population'])
df_population_mod_dropna = df_population_mod_dropna.dropna(subset=['Life_Expectancy'])
df_population_mod_dropna = df_population_mod_dropna.dropna(subset=['Female_Population'])
df_population_mod_dropna

"""* Observamos que solo reducimos en una fila nuestro dataset."""

## Validando que no exista presencia de valores nulos en el dataframe
df_population_mod_dropna.isna().sum()

"""* Ya no tenemos presencia de datos **NaN** en nuestro dataset.

### 2.2.2 Tranformando el tipo de dato de nuestras variables

---
"""

## Convertimos a int64 nuestras variables numéricas
df_population_mod_dropna['Female_Population'].astype('int64')
df_population_mod_dropna['Total_Population'].astype('int64')
df_population_mod_dropna['Male_Population'].astype('int64')
df_population_mod_dropna['Population_Density'].astype('int64')
df_population_mod_dropna['Life_Expectancy'].astype('int64')

## Verificando que la conversión haya sido exitosa
df_population_mod_dropna.info()

"""* Notamos que no llegó a convertirse a **int64**, debido a que poseemos tipo de datos **float64**. COntinuamos con este tipo de dato."""

## Multiplicamos por 1000 algunas variables, a fin de tener un dato más exacto sin decimales
df_population_mod_dropna[['Total_Population','Male_Population','Female_Population']] = df_population_mod_dropna[['Total_Population','Male_Population','Female_Population']]*1000
df_population_mod_dropna

## Validando no presencia de nulos nuevamente
df_population_mod_dropna.isnull().sum()

"""### 2.2.3 Dataframe Transformado"""

## Reasignando la variable y corrigiendo el índice
df_population_limpio = df_population_mod_dropna.reset_index(drop=True)
df_population_limpio

"""# 3. Planteamiento de Preguntas

---

## 3.1 ¿Cómo ha evolucionado el Covid-19 en México-Perú-Colombia en comparación con el impacto observado a nivel global?
"""

## Validamos nuestro dataframe donde responderemos la pregunta
df_covid_limpio_final.head(10)

"""* En esta parte, trataremos de conseguir la distribución de casos acumulados alrededor del mundo. Pero nos centraremos en el top 10 de países y adicional, analizaremos 3 países de Latinoamerica: México, Perú y Colombia."""

## Seleccionando los 10 paises con más casos acumulados - Top 10
top_10_countries= df_covid_limpio_final.groupby('Country').max().reset_index()[['Country', 'Cumulative_cases']].sort_values(by='Cumulative_cases', ascending = False).head(10)
top_10_countries

## Agrupando los países que no están en el top 10
other_countries = df_covid_limpio_final.groupby('Country').max().reset_index()[['Country','Cumulative_cases']].sort_values(by='Cumulative_cases', ascending=False).iloc[10:]
other_countries

## Conseguimos los 3 países que analizaremos el COVID 19: México - Perú - Colombia
countries_evaluated = other_countries[(other_countries['Country']== 'Mexico')| (other_countries['Country']== 'Colombia')| (other_countries['Country']== 'Peru')]
countries_evaluated

## Casos acumulados por los países que no están en el Top 10 ni tampoco son los 3 países que estamos analizando
other_countries_without_analized = other_countries['Cumulative_cases'].sum() - countries_evaluated['Cumulative_cases'].sum()
other_countries_without_analized

# Usando concat para agregar los 3 países a analizar al DataFrame top 10
top_10_countries = pd.concat([top_10_countries, countries_evaluated], ignore_index=True)

# Adicionando nueva fila (Other_countries) al DataFrame top_10_countries
new_row = pd.DataFrame([{'Country': 'Others', 'Cumulative_cases': other_countries_without_analized}])
top_10_countries = pd.concat([top_10_countries, new_row], ignore_index=True)
top_10_countries

## Modificando los nombres de algunos paises, por la longitud
top_10_countries['Country'] = top_10_countries['Country'].replace({'United States of America':'USA', 'United Kingdom of Great Britain and Northern Ireland':'United Kingdom'})
top_10_countries

## Gráfico de Pie
valores = top_10_countries['Cumulative_cases']
etiqueta = top_10_countries['Country']
explode = (0.1, 0.1, 0.03, 0.03, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.2, 0.2, 0.2, 0.02)
#colores = plt.cm.viridis((0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0))
data = [210, 200, 180, 170, 150, 140, 120, 100, 80, 70, 70, 60, 60, 50]
colors = plt.cm.Blues(data)
colors2 = plt.cm.viridis(data)

plt.figure(figsize=(15,10))
plt.pie(valores,labels=etiqueta, explode=explode,autopct='%1.1f%%', textprops={'fontsize': 11,'weight': 'bold'}, pctdistance=0.85,
        startangle=182, wedgeprops=dict(width=0.5, edgecolor='w'), labeldistance=1.05, colors=plt.cm.Set2.colors, shadow=True)

plt.title('Casos Distrubucion Mundial', fontdict={'fontsize': 27, 'fontweight': 'bold', 'color': 'Black',
    'family': 'sans-serif',
    'style': 'italic'})
plt.legend(etiqueta.T, bbox_to_anchor=((1.1,0.8)), frameon=True, edgecolor='gray')
plt.tight_layout()
plt.show()

"""* En la gráfica de pie notamos que los dos **paises con más casos acumulados** y por ende con mayor porcentaje, son **USA y China**, en ese orden respectivamente.
* El **resto de países que comprenden el Top 10**, tienen un porcentaje de casos acumulados similares que van del **5.8% al 3.2%**.
* Los 3 Países latinoamericanos analizados **(México-perú-Colombia)**, tiene porcentajes de casos acumulados mucho más bajos comparados con los demás países, va en el rango del **1% al 0.5%**.
* Considerar como dato importante la población de los países, este es un dato importante a tener en cuenta.

## 3.2 ¿Cuál ha sido la evolución de los nuevos casos diarios reportados de Colombia a lo largo del tiempo?

---
"""

## Realizamos una agrupación del dataframe por País y día reportado, seguido de la suma de los casos acumulados.
df_covid_limpio_final.groupby(['Country','Date_reported']).sum()['Cumulative_cases'].unstack()

## Hacemos uso de la función pivot, a fin de crear una tabla descriptiva de como se iba acumulando los casos para todos los paises
df_grouped = df_covid_limpio_final.pivot(index='Date_reported', columns='Country', values='Cumulative_cases').reset_index()
df_grouped

## Separamos solo los paises a evaluar: México-Perú-Colombia
df_grouped_evaluate = df_grouped[['Date_reported','Mexico','Peru','Colombia']]
df_grouped_evaluate

## Creamos nuevas columnas, son la diferencia de filas consecutivas de 'Cumulative_cases' a fin de conseguir los casos diarios
df_grouped_evaluate = df_grouped_evaluate.copy()
df_grouped_evaluate['Mexico_daily_cases'] = df_grouped_evaluate['Mexico'].diff().fillna(0)
df_grouped_evaluate['Peru_daily_cases'] = df_grouped_evaluate['Peru'].diff().fillna(0)
df_grouped_evaluate['Colombia_daily_cases'] = df_grouped_evaluate['Colombia'].diff().fillna(0)
df_grouped_evaluate

## Discriminamos el dataframe solo para analizar Colombia, consideramos solo casos diarios con valores mayores a cero
daily_cases_colombia = df_grouped_evaluate[['Date_reported','Colombia_daily_cases']]
daily_cases_colombia = daily_cases_colombia[daily_cases_colombia['Colombia_daily_cases']>0]
daily_cases_colombia

## Discriminamos el dataframe solo para analizar Perú, consideramos solo casos diarios con valores mayores a cero
daily_cases_peru = df_grouped_evaluate[['Date_reported','Peru_daily_cases']]
daily_cases_peru = daily_cases_peru[daily_cases_peru['Peru_daily_cases']>0]
daily_cases_peru

from matplotlib.dates import DateFormatter
import matplotlib.dates as mdates
plt.figure(figsize=(18,7))
plt.bar(daily_cases_colombia['Date_reported'],daily_cases_colombia['Colombia_daily_cases'],alpha=0.7, width=8)
plt.title('Casos Nuevos Diarios - Colombia', fontdict={'fontsize': 20, 'fontweight': 'bold', 'color': 'black'})
plt.ylabel('Millones', fontdict={'fontsize': 14, 'color': 'black'})

# Ajustar el límite inferior del eje x a enero de 2020
start_date = pd.to_datetime('2020-01-01')
end_date = daily_cases_peru['Date_reported'].max()

# Añadir formateo al eje Y con números enteros y "M" para millones
def millions_formatter(x, _):
    return f'{round(x/1e6, 2)}M'

# Formatear las fechas en el eje x y ajustar el intervalo
date_format = mdates.DateFormatter("%b %y")
plt.gca().xaxis.set_major_locator(mdates.MonthLocator())  # Mostrar fechas cada 30 días
plt.gca().xaxis.set_major_formatter(date_format)

plt.gca().yaxis.set_major_formatter(FuncFormatter(millions_formatter))

plt.xlim(start_date, end_date) # Establecer los límites del eje x
plt.grid(axis='x')
plt.xticks(rotation=35)
plt.tight_layout()
plt.show()

"""## 3.3 ¿Cuál es la evolución del índice de letalidad del Covid-19 en Colombia, comparado con los países con los índices históricos más elevados?

---


"""

## Verificamos nuestro dataframe en el cual realizaremos el análisis
df_covid_limpio_final

## Agrupamos los 5 paises con mayor lethality_rate promedio hasta estos momentos
top_5_lethality_rate=df_covid_limpio_final['lethality_rate'] = df_covid_limpio_final['Cumulative_deaths'] / df_covid_limpio_final['Cumulative_cases']
top_5_lethality_rate.head(20)

# Filtrar datos para México, Perú y Colombia
countries_of_interest = ['Mexico', 'Peru', 'Colombia']
df_covid_filtered = df_covid_limpio_final[df_covid_limpio_final['Country'].isin(countries_of_interest)]

# Verificar los datos filtrados
df_covid_filtered.head()

# Calcular el promedio del índice de letalidad por país
lethality_rate_by_country = df_covid_limpio_final.groupby('Country')['lethality_rate'].mean()

# Obtener los 3 países con el índice de letalidad más alto
top_lethality_countries = lethality_rate_by_country.sort_values(ascending=False).head(3).index

# Verificar los países con índices de letalidad más elevados
print(top_lethality_countries)

# Filtrar los datos para los países con índices de letalidad más elevados
df_top_lethality_countries = df_covid_limpio_final[df_covid_limpio_final['Country'].isin(top_lethality_countries)]

# Combinar con los datos de México, Perú y Colombia
df_combined = pd.concat([df_covid_filtered, df_top_lethality_countries])

# Verificar los datos combinados
print(df_combined.head())

# Graficar la evolución del índice de letalidad
plt.figure(figsize=(10, 4))

for country in df_combined['Country'].unique():
    df_country = df_combined[df_combined['Country'] == country]
    plt.plot(df_country['Date_reported'], df_country['lethality_rate'], label=country)

plt.xlabel('Fecha')
plt.ylabel('Índice de Letalidad')
plt.title('Evolución del Índice de Letalidad del COVID-19')
plt.legend()
plt.grid(True)
plt.show()

"""# 4. CONCLUSIONES FINALES

---

* Los países más afectados por el Covid19, fueron **USA y CHINA**, consiguiendo casos registrados de **covid 19** y decesos records comparados con los demás países. Tener en cuenta que un **factor importante** es la **densidad de población del país**, esto generó que estos países concentraran toda esta magnitud.


* **México**, tuvo un **periodo complicado** de la pandemia durante el **primer año y medio**. Fue afectada por varias variantes, pero una de las que generó picos de nuevos casos y decesos fue la variante **Delta**. Si bien los fallecidos también tuvieron picos en estos periodos, no fueron comparables a los picos conseguidos al inicio de la pandemia, quizás ocasionado por la no correcta adopción de medidas sanitarias y concientización de la población.


* **Perú**, tuvo un **periodo complicado** de la pandemia durante el **primer año y medio**, teniendo crisis sanitaria y hospitalaria fuerte. El proceso de vacunación fue tardado, esto alimento a que los decesos fueran no controlables en este periodo inicial. Después de este periodo complicado, perú puede controlar adecuadamente el **Covid19**, consiguiendo que gran parte de la población logre vacunarse y asi los decesos disminuyeron considerablemente hasta el final de la pandemia. La primeras variantes del covid19 afectaron mucho a perú, sin embargo, cuando llegó la variante **Omicrón**, se pudo controlar el número de decesos por más que los nuevos casos siguieron aumentando por intervalos de tiempo.


* **Colombia**, tuvo un **periodo complicado** de la pandemia durante el **primer año y medio**. Los nuevos casos como los decesos aumentaban a rates parecidos, esto generó descontrol en el país, con **crisis sanitaria y hospitalaria fuerte**. Despues de este periodo gris, colombia **pudo controlar medianamente el covid19**, debido a su aumento en la **vacunacion de la población**; Sin embargo algunas nuevas variantes como el **Omicrón**, generaron intervalos de preocupación y crisis.


* Cada país se vió afectado de distinta manera por las diferentes variantes del **Covid19**, no todas afectaron en misma magnitud a todos los países.


* Luego de analizar la data para los múltiples países, se puede concluir que el **proceso de vacunación**, cuando se empezó a adoptar si **tuvo resultados favorables** en los meses posteriores. Esto se puede visualizar en los **gráficos de nuevos casos como de decesos**
"""